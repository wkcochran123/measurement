\chapter{Phenomena}
Recall the thought experiment of the parked car transported across the country.
The displacement is large, the duration is long, and ordinary
reasoning would readily describe the episode as one of sustained motion.  Yet
the speedometer continues to record nothing at all.  Its silence is not a
failure of detection or a lack of sensitivity.  It is a faithful expression of
its construction.  Motion that does not pass through wheel rotation is
inadmissible to this instrument and therefore invisible to its ledger.

When the car is finally driven again, the wheel completes its next rotation and
the speedometer advances by exactly one count.  The instrument does not record
how long the car was idle, does not distinguish whether the pause lasted minutes
or decades, and does not reflect the intervening transport.  Its ledger
registers only the completion of a bounded exchange.  All intervening time and
motion lie outside its refinement path and therefore outside its history.

This example establishes a central lesson carried forward from the previous
chapter.  Instruments do not record what happens in general.  They record only
what their refinement structure permits.  Silence is not ignorance about hidden
activity; it is the absence of licensed distinction.  From the perspective of
the speedometer, the transported car has no history during the interval of
stillness, regardless of what may be inferred by other means.

However, consider another device capable of measuring the phenomenon we call
speed.  A global positioning system (or GPS) device does not refine
motion through mechanical cycles.  It refines position by receiving timed
signals from distant sources and committing the result as a coordinate.  Its
ledger advances by solving a synchronization problem rather than by waiting for
a wheel rotation.  Where the speedometer is silent, the GPS may remain active.

The GPS therefore records a history that the speedometer cannot.  During the
train ride, new position fixes may be committed, each summarizing a completed
internal computation.  These entries do not contradict the silence of the
speedometer, because they belong to a different ledger governed by a different
refinement scheme.

The two instruments do not disagree.  They simply speak in nonoverlapping
alphabets.  That this fact is easy to miss is itself instructive.  Both
instruments may display values labeled in the same units, such as kilometers
per hour, yet those labels conceal fundamentally different modes of
construction.  What appears as a common numerical value is, at the level of the
ledger, a projection from distinct symbolic processes.

Agreement in units does not imply agreement in records.  It signals only that
the two ledgers admit a common coarsening under which their outputs may be
compared.  The apparent equivalence of the displayed values is therefore not a
primitive fact, but a consequence of calibration.  It is achieved by suppressing
details that belong to one refinement scheme but not the other.

The problem addressed in this chapter is not how to choose between instruments,
nor how to privilege one account of motion over another. It is how the operation
of an instrument may be analyzed so as to uncover the phenomenon it presupposes.
An instrument does not merely produce records; it embodies a 
\emph{phenomenal hypothesis} about
what aspect of experience is stable enough to be measured at all.

What observers ultimately agree upon is therefore not a shared internal state,
nor a common notion of simultaneity, but the persistence of a phenomenal
structure across the records their instruments generate. That structure is not
written directly to any ledger. It is inferred from the consistency of recorded
histories when projected to a suitable level of description.

At this coarser level, familiar quantities emerge. Speed is not located in the
wheel rotation count, nor in the satellite timing solution. It appears as a
phenomenal invariant: a relation that remains stable across the union of moments
produced by each instrument when their records are decomposed. Each ledger
refines this invariant differently, and neither refinement exhausts the
phenomenon itself.

This perspective reverses the usual explanatory order. Instruments do not
measure speed because speed exists as a primitive quantity. Rather, speed is
identified as that aspect of experience which survives the distinct refinement
paths imposed by different instruments. The phenomenon is uncovered only after
the instrument has been decomposed and its invariant extracted.

Physical law enters the framework at this point, but in a limited role. Laws do
not generate motion, nor do they dictate what an instrument must see. They act as
bookkeeping constraints that preserve coherence once a phenomenon has been
identified. A law summarizes how an invariant behaves under translation between
records; it does not supply the invariant itself.

The GPS does not correct the speedometer, and the speedometer does not invalidate
the GPS. Each instrument is faithful to its own phenomenal hypothesis and its
own mode of refinement. What matters is that, when their records are analyzed,
they point to the same underlying phenomenon. Calibration is the discipline by
which this agreement is made explicit.

The remainder of this chapter develops the structures required to uncover such
phenomena. It formalizes how an instrument may be decomposed, how invariants are
extracted from its operation, and how silence and activity in the record bound
the scope of what the instrument can meaningfully claim to measure.

By focusing on the phenomenon behind the instrument, rather than on agreement
between instruments, the framework maintains its evidentiary stance. Phenomena
are not assumed; they are inferred from the stability of what survives
measurement. Only after this inference is complete does comparison between
instruments become possible, and only then do laws acquire their organizing
role.

\section{Invariant}

What is perceived in a phenomenon is not its totality, but its persistence.
Experience does not present itself as an unstructured flux. Certain relations
remain stable while others vary, and it is this stability that makes recognition
possible at all. Invariance is therefore not a secondary abstraction imposed by
analysis, but the primary feature through which a phenomenon is apprehended.

A phenomenon is identified by what does not change when conditions do. Motion
is recognized through constancy of displacement over time, color through
stability of response under illumination, and pitch through persistence across
repetition. In each case, perception latches onto relations that survive
variation. What fluctuates is relegated to background; what persists is taken
to characterize the phenomenon itself.

This persistence precedes any act of measurement or recording. One does not
first measure and then infer invariance; rather, measurement is undertaken
because invariance is anticipated. The expectation that something will remain
stable under repeated interaction is what licenses the construction of an
instrument in the first place. In this sense, invariance is the condition for
measurement, not its product.

Within the framework developed here, invariance is made explicit through
interaction with a device. Measurement does not
create invariance, but exposes it by suppressing distinctions that fail to
persist. What survives this suppression is reconstructed as the invariant
structure of the phenomenon. All subsequent formal notions of record,
instrument, and decomposition are refinements of this basic perceptual fact.

This exposure of invariance does not depend on the material form of the
disturbance with which a device interacts. What matters is that interaction
with the carrier yields events whose relations vary in a structured way under
relative motion. The device does not recover the carrier itself, but registers
systematic shifts in the received symbols as conditions change.

When such shifts are stable under repetition, they define a phenomenon. The
underlying mechanism by which the disturbance propagates becomes secondary to
the pattern of variation it supports. What is preserved is not the form of the
carrier, but the relation between emission, reception, and relative motion that
the carrier makes observable.

It is in this sense that distinct physical interactions may support the same
phenomenon. If interaction with a propagating disturbance produces comparable
patterns of change in recorded events, then those interactions are coordinated
by a common carrier structure. The phenomenon is identified by this shared
structure, not by the substrate through which it is realized.

For instance, sonar and radar are often presented as fundamentally different forms of
detection. Sonar propagates through water by means of pressure oscillations in
a material medium. Radar propagates through vacuum by means of electromagnetic
disturbances that do not appear to require a material substrate. One seems to
involve matter directly; the other seems to involve pure field.

From the perspective of the ledger, however, this contrast is superficial.
In both cases the instrument records emission events and reception events.
Between those entries no intermediate states are committed. The instrument
does not append a continuous history of the carrier’s passage. It appends only
that a signal was sent and that a signal was received, together with relations
among those events.

In sonar, one may indeed measure the water itself. Temperature gradients,
salinity, and density fluctuations may be recorded by additional devices.
These measurements describe the medium. But they are separate ledgers,
generated by separate instruments. They do not appear automatically as part
of the sonar record.

In radar, the absence of a tangible medium is sometimes taken to imply that the
electromagnetic field must itself be the carrier. Yet the field equations used
to model radar are internal to the instrument. They guide decomposition,
calibration, and prediction, but they do not enter the ledger. What the radar
device records are discrete events: emission, echo, timing, and frequency
shift. The intervening field is a representational structure, not a committed
fact.

\subsection{Carrier}

This observation clarifies a structural distinction. A device relies on a
carrier, not on a medium. A carrier is whatever structured disturbance gives
rise to correlated, admissible events at distinct devices. A medium, by
contrast, is an explanatory model for how that disturbance propagates between
events. The ledger requires the former in order to record; it does not require
the latter in order to remain coherent.

This distinction becomes sharper in light of Galilean relativity. In classical
mechanics, the laws governing motion are invariant under uniform translation of
the reference frame. Whether a ship moves through still water or the water
moves past a stationary ship, the relations among admissible events may be
preserved. The medium may possess motion; the invariant relations between
events need not.

For sonar, this is explicit. A current may flow through the water, altering
local conditions. Yet when emission and reception events are placed,
the propagation constraint may remain stable. What changes is the
model of the medium; what persists is the event relation certified by the
ledger.

The same structural feature appears in radar. Whether one models electromagnetic
propagation relative to a stationary observer or to one in uniform motion, the
invariant relations among admissible events are preserved.  The medium is absent or 
reinterpreted; the invariant remains.

Galilean relativity thus reinforces the separation between carrier and medium.
Uniform motion of the explanatory substrate does not alter the structural
relation between emission and reception events. The invariant does not reside
in the medium; it resides in the stability of admissible event relations under
change of representation.

This recognition was extended from mechanics into mathematics by Abel. In his
analysis of algebraic equations, Abel observed that the solvability of a
polynomial does not depend on the particular form in which it is written, nor
on the coordinates used to express its roots. Transformations may alter the
representation, but certain structural properties remain invariant. It is these
properties, rather than the symbolic surface, that determine what may be
recovered.

Abel’s work marked a decisive shift from the study of explicit expressions to
the study of relations preserved under transformation. The object of inquiry
ceased to be the formula itself and became the structure that survives
permutation, substitution, and reparameterization. In this sense, abstract
mathematics begins where invariance becomes primary. Just as Galilean relativity
separates physical law from the state of the medium, Abel separates algebraic
structure from the symbols that temporarily carry it. What matters is not the
particular representation, but what remains stable when representation changes.

\begin{phenom}{The Galileo--Abel Effect~\cite{galilei1632,abel1826}}
\label{ph:invariance}

\PhStatement
Invariant structure is preserved under admissible transformation of
representation. Changes in frame, coordinate, or symbolic form may alter the
carrier or medium of description without altering the structural relations
certified by the ledger.

\PhOrigin
Galileo observed that the laws of mechanics remain unchanged under uniform
translation of the reference frame~\cite{galilei1632}. Motion of the ship or
motion of the water yields equivalent physical relations when described from
different frames. Abel later demonstrated that solvability of
algebraic equations depends not on the particular expression of a polynomial,
but on structural relations preserved under transformation~\cite{abel1826}.
Both recognized that what survives change of representation is more fundamental
than the representation itself.

\PhObservation
Instruments may model propagation through media, invoke coordinate systems, or
manipulate symbolic expressions. Yet admissible events recorded in the ledger
exhibit stable relations that persist under these transformations. The medium
may change, the coordinates may shift, and the formula may be rewritten; the
invariant relation remains recoverable.

\PhConstraint
No transformation is admissible if it alters the invariant relations among
recorded events. Representation may vary only insofar as recovery of the ledger
is preserved. Changes that introduce distinctions not supported by recorded
events are unlawful refinements.

\PhInvariant
\emph{Invariance} The invariant is the stable relation among admissible events
that survives change of carrier, coordinate, or symbolic form. It is not
identified with the medium, nor with any particular representation, but with
the recoverable structure that persists across ledger refinements.

\PhRefinement
Under refinement, representations may become more detailed, coordinate systems
more precise, and decompositions more elaborate. The invariant persists only if
these refinements preserve recoverability of prior records. The Galileo--Abel
Effect therefore refines Phenomena~\ref{ph:kant-effect} and
\ref{ph:clock} by elevating invariance itself to the primary object of
measurement.

\PhConsequence
Phenomenon~\ref{ph:invariance} licenses abstraction. Physical law is separated from
the state of the medium, and mathematical structure is separated from symbolic
form. The invariant becomes the primary object of study. What appears as motion
of substance or manipulation of symbols is revealed to be transformation of
carrier, while the structural residue certified by the ledger remains fixed.
\end{phenom}


The distinction is structural. A carrier is defined operationally by its
capacity to produce correlated events. A medium is defined
theoretically by equations describing metaphysical states between those events.
The carrier is physical in the instrumental sense; the medium is mathematical
in the representational sense.

This difference clarifies a recurring confusion between the real and the
modeled. The real content of sonar is the stable relation between emission and
reception events. The mathematical content includes wave equations and boundary
conditions imposed on a fluid. Likewise, the real content of radar is the
timing relation between transmitted and received pulses. The mathematical
content includes Maxwell’s equations and relativistic corrections. The former
is ledger-stable; the latter is instrument-internal.

Any semblance of a continuous medium arises only when one attempts to explain
why the invariant relation holds. That explanation is constructed from other
devices. Thermometers, pressure gauges, interferometers, and spectrum analyzers
produce additional ledgers that may be brought into correspondence with the
sonar or radar record. The appearance of a medium is therefore mediated by
further measurement.

\subsection{Simultaneity}

A single phenomenon may employ multiple carriers. A vehicle’s speed may be
registered by the rotation of a wheel in a speedometer, by the reflection of a
radar pulse, and by timing differences among photons exchanged with satellites
in a GPS system. These carriers differ in mechanism, propagation, and internal
decomposition. Yet the events they generate may be placed into correspondence.

In each case, an instrument records admissible events: a wheel completes a
rotation, a radar echo returns, a timing correction is computed from satellite
signals. These events occur within distinct devices and are ordered internally
according to each instrument’s refinement structure. The local ordering is not
identical across devices.

Nevertheless, the observer may identify a common phenomenal invariant. The
speed inferred from wheel rotations, the Doppler shift measured by radar, and
the velocity estimate derived from GPS timing are all recognized as
realizations of the same invariant relation among admissible events. 

This is simultaneity. To say that the three instruments register a
change ``at the same time'' is not to assert that their internal events are
strictly ordered in the same way. Mechanical compliance, signal latency, and
propagation delay ensure that exact temporal coincidence is neither expected
nor meaningful within each device.

Rather, simultaneity expresses indifference to the local ordering of certain
events. If one device records $a$ before $b$ while another records $b$ before $a$, 
yet both correspond to the same change in the phenomenal
invariant, the ordering becomes secondary. The invariant relation is preserved
despite variation in event sequence.

For instance, a radar gun measures speed at the discretion of an observer who
initiates emission of a photon. Yet the photon is only the first element in a
long chain of mechanisms. The reflected signal must be received, amplified,
filtered, digitized, transformed, averaged, and finally converted into a
numerical display. Each stage introduces delay and internal ordering of events.
The number that appears on the screen is the endpoint of a structured
decomposition that unfolds over time within the device.

The speedometer is no simpler. Wheel rotation drives a sensor that generates
electrical pulses, which are counted, integrated, filtered, and translated into
a display value. Mechanical inertia, sampling intervals, and computational
smoothing introduce their own latencies. The number shown on the dashboard is
not the instantaneous state of a spoke at a particular angle, but the output of
a refined internal process.

Between the emission of the radar pulse and the stabilization of its display,
the wheel continues to rotate and the speedometer continues to update. The
internal events of the two devices interleave in complicated ways, with no
global synchronization. Yet the invariant they report may coincide. The
precise temporal alignment of photon arrival with wheel phase is irrelevant to
the recovered speed. What matters is that both chains of mechanism project to
the same phenomenal relation. The invariant survives the
independent delays, transformations, and internal orderings of each carrier.

This coincidence is not achieved by synchronizing mechanisms, nor by enforcing a
shared temporal order. Each device evolves according to its own internal clock,
and the carriers they interact with propagate under distinct physical
constraints. What permits agreement is not simultaneity, but the existence of a
relation that cannot be decomposed into independent local accounts without loss.
The invariant binds together outcomes that arise from separate chains of
interaction.

In this sense, the recovered speed is not a property of the wheel alone, nor of
the radar pulse alone. It is a property of the joint structure induced by their
interaction with a common phenomenon. Attempting to assign it to either device
in isolation destroys the relation that defines it. The phenomenon is therefore
intrinsically composite, even though its manifestations are local.

This structural feature is not unique to macroscopic measurement. It reflects a
more general principle: when admissible outcomes are constrained by a shared
invariant, they form a configuration whose structure exceeds that of its
individual components. The inability to factor such structure into independent
parts is the hallmark of a deeper mode of description.

Schrodinger recognized that the defining feature of quantum systems is not the
behavior of isolated particles but the structure of composite states. In his
response to the EPR argument, he introduced the notion of entanglement to
describe systems whose admissible outcomes cannot be factorized into
independent components. The state of the whole cannot be reduced to the states
of its parts without loss of structural information. Correlation is therefore
not accidental but intrinsic to the configuration of admissible distinctions.

This shift displaced the classical emphasis on localized sequence. In an
entangled system, the ordering of measurements performed on spatially
separated subsystems does not alter the recoverable invariant relation between
their outcomes. The structure of correlation
persists despite variation in local event order. What matters is not which
measurement occurred first, but that the admissible distinctions were fixed in
a particular configuration prior to refinement.

A similar structural lesson appears in the dual-slit experiment. When two
paths are available but no which-path information is recorded, the admissible
event structure does not separate the alternatives into independent classes.
The contributions of the two paths remain structurally simultaneous. Only when
additional refinement distinguishes the paths does the interference pattern
disappear. It is this dependence on simultaneity of admissible alternatives
that prepares the ground for the Schrodinger--Young Effect.

\begin{phenom}{The Schrodinger--Young Effect~\cite{young1804,schrodinger1935}}
\label{ph:sy}

\PhStatement
Structural correlation among admissible events may persist independently of
classical ordering. The availability or suppression of distinctions alters the
admissible invariant, while local event sequence may remain secondary.

\PhOrigin
Young demonstrated that interference patterns arise from the structural
configuration of available paths rather than from localized particle
trajectories~\cite{young1804}. The resulting pattern depends on whether
distinctions between paths are admitted or suppressed. Schrodinger later
identified entanglement as the defining structural feature of composite
systems, emphasizing that correlated outcomes cannot be reduced to independent
local descriptions~\cite{schrodinger1935}. In both cases, structure supersedes
classical sequencing.

\PhObservation
In the dual-slit experiment, introducing which-path information alters the
recoverable interference pattern even when no continuous trajectory is recorded.
The invariant depends on the admissible distinction structure, not on a
chronological history of localized events.

In entangled systems, spatially separated measurements may be performed in
different local orders, yet their outcomes exhibit invariant correlation.
The structural relation persists despite variation in temporal
sequence within individual devices.

\PhConstraint
Ordering distinctions are admissible only insofar as they alter the recoverable
invariant. If differing local sequences project to the same invariant relation,
the ordering is representational rather than structural.
Conversely, the introduction of new distinctions that change admissibility
modifies the invariant itself.

\PhConsequence
The Schrodinger--Young Effect establishes that simultaneity is structural
equivalence, not coincidence in time. Phenomena defined by
multiple carriers cannot be reduced to a single ordered sequence of events.
Correlation and admissibility determine the invariant; temporal ordering is
secondary to recoverable structure.
\end{phenom}

Interference arises precisely when alternative event contributions are
structurally simultaneous. In the dual-slit configuration, the two admissible
paths correspond to distinct local decompositions of the same phenomenon. When
no which-path distinction is recorded, the ledger does not refine these paths
into separate event classes. They remain simultaneous at the level of the
phenomenon. The resulting invariant therefore reflects their joint structural
contribution rather than an ordered sum of independent trajectories.

This simultaneity is not chronological coincidence but indifference under
projection. The admissible events associated with each path cannot be
separated without introducing additional distinctions. So long as the ledger
does not commit such distinctions, the paths remain structurally co-present.
Their contributions combine before refinement resolves them, producing the
interference pattern as a property of invariant correlation.

When which-path information becomes available, simultaneity is broken.
Refinement separates the previously co-present alternatives into distinct
event classes. Once distinguished, the alternatives admit inversion: one may
assign outcomes to one path or the other, and the interference structure
disappears. The invariant changes because the admissible structure has been
altered. Lack of simultaneity permits decomposition into ordered components,
and the phenomenon transitions from interference to mixture.

This inversion is not arbitrary. It is licensed only when both orderings
project to the same admissible refinement of a common ledger. Simultaneity is
therefore not primitive equality of timestamps, but equivalence under
projection. Two events are simultaneous with respect to a phenomenon if their
difference does not alter the recoverable invariant.

\subsection{Temporal Friction}

The Einstein Effect binds emission and reception into a single invariant
structure. Yet there are situations in which emission may be ignored and the
reception phenomenon alone becomes decisive. In such cases, the geometry of
propagation is fully specified by model, but the admissible event remains
temporally under-determined. The invariant relation is known; the ledger must
still decide.

Consider the operation of GPS. A receiver measures the arrival times of
signals from satellites whose worldlines are precisely modeled. 
In principle, three satellites determine a position by
intersection of three spheres in space. The mathematics is complete and the
geometry is specified.

Yet with three satellites, the intersection generally produces two possible
solutions. One lies on or near the surface of the Earth. The other lies far
above it, displaced into space. Both satisfy the geometric equations. Both are
consistent with the modeled propagation delays. The invariant relation among
arrival times does not uniquely determine position.

At this stage, the system exhibits temporal friction. The model provides
multiple admissible refinements compatible with recorded reception events.
Selection cannot proceed purely from geometric decomposition. The ledger must
be consulted.

In practice, the receiver resolves this ambiguity by incorporating additional
information. The fact that the device is constrained to the Earth’s surface
may be imposed as an external admissibility condition. Alternatively, the
arrival of a fourth satellite provides an additional timing constraint,
eliminating the extraneous intersection and collapsing the ambiguity.

When the fourth signal is received, the temporal under-determination vanishes.
The extra constraint removes the residual bit of error in the receiver’s
clock. What previously appeared as two admissible positions reduces to one.
The invariant relation is sharpened by refinement of the ledger.

This illustrates a general principle. A model may fully specify the geometry
of propagation and still leave temporal ambiguity unresolved. In such cases,
the invariant is not altered, but its realization remains incomplete. The
ledger must accumulate sufficient reception events to disambiguate among
geometrically admissible alternatives.

Temporal friction therefore describes the resistance encountered when a
phenomenon is specified by model more finely than by recorded events. The
difference between modeled time and committed time appears as multiplicity of
solutions. The ledger acts as the arbiter, selecting the refinement consistent
with accumulated admissibility.

Dirichlet recognized that differential equations do not determine phenomena
without supplementary specification. The governing relation may be exact and
internally consistent, yet admit infinitely many solutions. What selects among
these possibilities is not the equation itself, but the imposition of boundary
or initial conditions. The solution becomes unique only once admissible data
are supplied at the boundary of the domain.

In the formulation of initial value problems, the same principle appears.
A differential law describes how a quantity evolves, but without specification
of its value at a given time, the evolution remains under-determined. The
equation constrains change; the initial condition commits the ledger. Together,
they yield a unique trajectory. Without the initial commitment, multiple
temporal realizations satisfy the same governing relation.

The Dirichlet perspective thus separates geometry from admissibility. The
equation encodes structural possibility; the boundary condition encodes
historical commitment. In the GPS example, relativistic geometry specifies the
propagation relation exactly, yet without sufficient temporal constraints the
position remains ambiguous. Additional reception events function as boundary
data, eliminating spurious solutions and restoring uniqueness. The ledger,
like the boundary condition in Dirichlet’s analysis, resolves what the model
alone cannot.


Bancroft’s algebraic treatment of the GPS equations makes this ambiguity
explicit~\cite{bancroft1985}. Even when the satellite positions are fully
specified and relativistic corrections are incorporated, satisfying
Dirichlet’s requirement that the boundary data be fixed, the system of
pseudorange equations may admit multiple solutions when insufficient reception
constraints are imposed. The algebra does not fail; it produces two
geometrically valid intersections consistent with the recorded arrival times.
The propagation model is exact, and the satellite worldlines are known, yet
uniqueness is not guaranteed.

\begin{phenom}{The Dirichlet--Bancroft Effect~\cite{dirichlet1850,bancroft1985}}
\label{ph:db}

\PhStatement
A fully specified geometric model may admit multiple admissible solutions
until sufficient boundary or reception constraints are committed to the
ledger. Uniqueness arises not from the governing equations alone, but from
additional admissible data.

\PhOrigin
Dirichlet formalized the role of boundary conditions in determining unique
solutions to differential equations~\cite{dirichlet1850}. A governing relation
may be exact and internally consistent, yet without specified boundary data it
admits multiple realizations. Bancroft later demonstrated that the algebraic
system underlying GPS position determination similarly admits multiple
solutions when insufficient constraints are imposed~\cite{bancroft1985}. In
both cases, structural completeness of the model does not guarantee uniqueness
of realization.

\PhObservation
In GPS, satellite positions and relativistic corrections are fully specified.
The pseudorange equations are exact. Yet with only three satellite receptions,
the system generally produces two geometrically valid intersections. One lies
near the Earth’s surface; the other lies far into space. Both satisfy the
modeled propagation relations.

In practice, it is rare that the receiver is located in space. Implementations
often incorporate admissibility assumptions that constrain the solution to the
Earth-bound branch, effectively eliding the extraneous realization. The ledger
is supplemented either by additional reception events or by environmental
constraints that function as boundary data.

\PhConstraint
No geometric specification alone suffices to eliminate multiplicity when
temporal parameters remain unresolved. Only additional committed events or
boundary admissibility conditions may collapse the solution space to a unique
realization.

\PhInvariant
The invariant is the propagation relation encoded in the pseudorange equations.
This relation remains stable across all admissible solutions. Multiplicity
arises not from failure of the invariant, but from insufficient refinement of
temporal commitment.

\PhRefinement
Under refinement—such as reception from a fourth satellite or incorporation of
environmental boundary constraints—the residual temporal degree of freedom is
eliminated. The extraneous branch disappears, and the solution becomes unique.
Refinement sharpens realization without altering the invariant.

\PhConsequence
The Dirichlet--Bancroft Effect demonstrates that model completeness and ledger
sufficiency are distinct. Even when geometry and relativistic corrections are
exact, admissible history must supply the final constraint. Uniqueness is not
imposed by equations alone, but by the accumulation of committed events.
\end{phenom}


The ambiguity arises from an unresolved temporal parameter in the receiver’s
clock. With only three satellites, one degree of freedom remains, and the
system bifurcates into two admissible realizations. Only when an additional
constraint is introduced—such as reception from a fourth satellite—does the
solution collapse to a single Earth-bound position. The geometry is complete,
but the ledger has not yet committed sufficient temporal information. One more
event removes the extraneous branch and restores uniqueness.

In GPS, the geometry of relativity is exact, yet a single unresolved timing
parameter produces bifurcation of position. Only through further reception is
the ambiguity resolved. The phenomenon demonstrates that temporal structure
is not exhausted by geometric specification. The ledger’s accumulation of
events is required to eliminate under-determined alternatives.

Temporal friction thus marks the boundary between model completeness and
ledger sufficiency. The Einstein Effect specifies invariant propagation.
The reception phenomenon reveals when additional refinement is required.
Only when geometry and ledger coincide does the invariant admit a unique
realization.

\subsection{Translations}

Interaction with a carrier produces events, but it does not by itself determine
their origin. An event records that something was received, not what was sent.
Inversion is the operation by which this question is addressed. Given a recorded
event, inversion asks which emissions are compatible with its occurrence under
the constraints of the carrier.

This problem arises prior to calibration or coordination between instruments.
It is forced by the asymmetry of interaction itself. Emission precedes
reception, but reception is all that is recorded. The carrier mediates between
the two, transforming what is sent into what is received. Inversion is the act
of traversing this mediation in reverse.

A carrier may therefore be understood as a dictionary. It associates possible
emissions with possible receptions, but the association is not generally
one-to-one. Multiple emissions may lead to the same received event, and some
emissions may fail to produce any record at all. The carrier defines which
translations are admissible, not which are unique.

Inversion does not recover a single prior state. It reconstructs a set of
admissible candidates consistent with the recorded event. Where the carrier
collapses distinctions, inversion reflects that collapse as multiplicity. This
non-uniqueness is not error or noise; it is the explicit representation of
information that was never committed to the ledger.

The structure of the carrier determines the structure of inversion. If the
carrier preserves distinctions, inversion is sharp. If it suppresses them,
inversion is broad. The degree of ambiguity in inversion is therefore a measure
of what the carrier forgets, not a failure of inference.

\begin{definition}[Inversion]
An \emph{inversion} is a decomposition whose factors are exchanged.
\end{definition}

Inversion is essential for identifying invariants. An invariant is not tied to a
specific emission or a specific reception, but to what persists across all
admissible inversions. Only by examining what survives reconstruction can one
distinguish structural relations from incidental detail.

At this stage, inversion is local and device-relative. It does not require
comparison with other instruments, nor alignment of records. It is a property
of the interaction between a single device and its carrier. Later chapters will
show how inversions may be coordinated, but here inversion serves only to expose
the structure imposed by the carrier itself.

\section{Instrument}

An instrument is the site at which a carrier is made legible. It does not
discover invariance, nor does it impose structure upon the world. It constrains
interaction so that certain distinctions are preserved while others are
suppressed. What an instrument produces are events: discrete marks that testify
to reception under repeatable conditions.

Different instruments may interact with the world through radically different
mechanisms. One may rely on mechanical rotation, another on acoustic reflection,
another on electromagnetic propagation. These mechanisms differ in material,
scale, and complexity. Yet the role of the instrument is not to record those
mechanisms. It is to register how reception changes under controlled variation.

What is registered is not the carrier itself, but the effect of interaction with
it. An instrument does not commit waveforms, fields, or trajectories to the
ledger. It commits symbols whose variation reflects relative motion between
emission and reception. The internal details of propagation are suppressed so
long as their influence on recorded events remains stable.

This suppression is not a defect but a necessity. Only by discarding information
that fails to persist can an instrument expose structure that does. The
instrument therefore acts as a filter: it admits distinctions that vary
systematically with interaction and rejects those that do not. What survives
this filtering is the phenomenal content of the measurement.

Across a wide range of instruments, the same pattern recurs. Relative motion
between source and receiver manifests as a systematic shift in recorded symbols.
The form of the shift may differ, but its role does not. Whether the instrument
counts rotations, intervals, echoes, or cycles, what is exposed is a relation
between emission and reception that remains stable under repetition.

This relation does not depend on the medium through which interaction occurs.
It does not depend on the internal ordering of events within the instrument.
It does not depend on synchronization between devices. It depends only on the
existence of a carrier that translates relative motion into a recoverable change
in recorded symbols.

For this reason, the diversity of instruments does not imply a diversity of
carriers at the phenomenal level. Distinct physical interactions may project to
the same carrier structure when they expose the same invariant relation. The
instrument serves only to realize this projection; it does not define the
relation itself.

The carrier singled out by this process is characterized entirely by how
relative motion alters admissible events. It is indifferent to substrate,
mechanism, and scale. Its defining feature is that it supports a stable mapping
between what is sent and what is received, such that variation in that mapping
encodes motion.

This carrier is sufficient. Any instrument capable of exposing relative motion
does so by engaging it. Additional carriers introduce no new phenomenal
structure; they merely offer alternative realizations. At the level of
invariance, they collapse to a single role.

Thus the instrument, regardless of its construction, serves to engage the same
carrier and to expose the same invariant. The apparent multiplicity of devices
and mechanisms conceals a deeper unity: there is only one way for motion to
become perceptible as structure, and only one carrier is required to support it.

\subsection{Waves}

Waves are the most familiar realization of a carrier. They present themselves
as extended disturbances that propagate through space, vary continuously, and
admit rich mathematical description. For this reason they are often taken to be
the paradigm of transmission. Yet the role waves play in measurement is more
restricted than their descriptive apparatus suggests.

An instrument interacting with a wave does not record the wave itself. It does
not commit the continuous profile of the disturbance, nor the detailed state of
the medium through which it propagates. What is recorded are discrete events:
arrivals, crossings, extrema, or intervals. The wave serves only as the vehicle
that produces these events.

From the perspective of the ledger, a wave functions as a generator of
repeatable markers. Peaks, zero crossings, and phase advances are treated not
as extended objects, but as countable occurrences. The continuity of the wave is
suppressed, and only those distinctions that can be stably repeated under
interaction survive.

In this way, wave-based measurement already exhibits particle-like structure.
The instrument responds not to the field as a whole, but to localized features
that admit enumeration. What appears continuous at the level of propagation is
encountered discretely at the level of reception. The wave hides a particle-like
role by distributing it across space and time.

This hiding is not accidental. It is enforced by the carrier itself. Any
propagating disturbance that supports stable event generation under repetition
will induce the same behavior. Whether one speaks of crests, pulses, or cycles,
the instrument ultimately engages a sequence of distinguishable occurrences.
The apparent particle is not an additional entity, but the event structure
exposed by interaction with the carrier.

\begin{phenom}{The Descartes Effect~\cite{descartes1637}}
\label{ph:Quantization}

\PhStatement
Continuous motion becomes measurable only after it is resolved into
ordered coordinates.  A trajectory is rendered admissible by being
quantized against fixed axes.

\PhOrigin
In \emph{La Geometrie}~\cite{descartes1637}, Descartes proposed that the
motion of a point on a surface may be described by assigning to it two
numbers determined by perpendicular lines.  A fly upon the ceiling,
previously describable only by gesture or narrative, becomes locatable
once its position is expressed as a pair of distances from chosen
references.  Motion is thereby decomposed into successive coordinate
pairs.  The path is not altered; its description is discretized.

\PhObservation
The ceiling is continuous, and the fly moves without visible jumps.
Yet the instrument that records its motion does not admit the ceiling
as such.  It admits only pairs of numbers drawn from calibrated scales.
Each recorded event is therefore a quantized instance of position.
Continuity survives only as the limiting coherence of these discrete
records.

\PhConstraint
No coordinate assignment is lawful unless it preserves recoverability
of position.  The axes must be fixed, the unit declared, and the
assignment repeatable.  Arbitrary refinement is prohibited if it
destroys comparability between successive records.  Quantization must
remain subordinate to the invariant trajectory it seeks to certify.

\PhInvariant
The invariant is the geometric relation among recorded positions.
The path, not the individual coordinate values, survives change of
origin, rotation of axes, or uniform rescaling.  The quantized ledger
entries vary; the structural curve remains recoverable.

\PhRefinement
Increasing precision refines the scale of admissible coordinates,
producing a denser ledger.  In the limit, the discrete succession of
records approaches a continuous curve.  The Descartes Effect therefore
licenses analytic geometry: continuous structure emerges as the stable
limit of ordered quantized events.

\PhConsequence
Phenomenon~\ref{ph:descartes-quantization} establishes quantization as
an act of calibration rather than an ontological claim.  The world is
not asserted to be discrete; measurement is compelled to proceed
discretely.  The device that records motion enumerates coordinate pairs.
From this enumeration arises geometry, calculus, and the possibility of
treating motion as a function.  Quantization precedes analysis.
\end{phenom}


Relative motion between source and receiver alters how these events are
encountered. The spacing, timing, or ordering of recorded occurrences shifts in
a systematic way. The instrument does not infer this shift from the wave’s
equations, but from the change in its own event record. What matters is not the
existence of a wave, but that the carrier supports such structured variation.

Thus waves do not introduce a new carrier. They realize the same carrier role in
a continuous disguise. The particle-like behavior revealed in measurement is not
evidence of an underlying ontology, but a consequence of how waves present
recoverable structure to an instrument. At the phenomenal level, waves collapse
to the same carrier already identified: one that translates relative motion into
stable, countable change.

\subsection{Representation}

The Descartes Effect establishes that motion becomes admissible only
after quantization.  A continuous ceiling is
replaced by ordered coordinate pairs; a trajectory becomes a sequence.
Quantization therefore supplies symbols.  It does not yet supply
structure.

Representation begins when these quantized events are organized so that
refinement becomes meaningful.  A ledger of coordinate pairs may be
longer or shorter, coarser or finer, yet nothing in the mere list of
pairs guarantees coherence among them.  To represent is to impose a
discipline on this enumeration so that successive refinements approach
a stable relation.

Two distinct operations are required.

First, decomposition partitions the admissible space.  The Cantor
construction provides the canonical example: an interval is refined by
nested subdivision, producing a hierarchy of sets whose intersection
recovers a point.  Partition alone, however, yields only a static
stratification.  It describes how space is cut, not how approximation
improves.

Second, iteration orders successive approximations.  Cauchy supplied the
criterion: refinement is lawful only if discrepancy decreases under a
declared norm.  Iteration without partition drifts; partition without
iteration fragments.  Representation requires both.

The Fourier method reveals their coordination.  In solving Laplace's
equation with prescribed boundary data, one decomposes admissible
functions into trigonometric modes and iteratively adjusts coefficients
so that boundary residue diminishes.  Each finite truncation is a
quantized representation; each coefficient update tightens the Cauchy
condition.  The harmonic solution is the limit certified jointly by
nested decomposition and convergent iteration.

Thus representation is not an ontological claim about continuity.  It is
a calibrated agreement between two refinements: one that partitions the
domain, and one that contracts error.  When these refinements commute,
a stable field emerges as invariant residue.

Quantization provided symbols.  Representation orders those symbols so
that refinement becomes convergent.  From this coordination arises the
possibility of treating motion as a function and a field as a fixed
point.  Continuity is not assumed; it is recovered as the limit of
disciplined enumeration.

\subsection{Iteration}

Chapter~2 established that repetition is not redundancy.  A repeated
trial is the first refinement of a fact.  The ledger grows, and with its
growth emerges accumulation.  Mean and dispersion arise not from a
single admissible event, but from the disciplined repetition of that
event under fixed calibration.  Repetition therefore generates a
numerical invariant.

In the present chapter, iteration plays the same structural role, but
now at the level of representation.  Where Chapter~2 repeated trials in
time, Chapter~3 repeats decompositions in structure.  Each step in a
Cauchy sequence is a renewed commitment to the same invariant under
increasing refinement.  The iteration is not searching for a new
quantity; it is tightening the certification of the existing one.

This parallel is exact.

A repeated trial appends another admissible event to the ledger.
A Cauchy iteration appends another admissible refinement to the
decomposition.  In both cases, discrepancy must diminish under a
declared norm.  In Chapter~2 the norm measured dispersion among trials.
Here the norm measures residue between successive approximations of the
field.  The logic is identical: no refinement is lawful if it enlarges
the certified error.

The Cantor decomposition supplies the structural scaffold.  Nested
partitions define progressively smaller admissible sets whose
intersection isolates a single value.  The Cauchy condition guarantees
that successive representatives drawn from these sets converge.  The
limit is not introduced as a metaphysical primitive; it is the invariant
that survives the infinite repetition of lawful refinement.

Thus the iteration that generates the single invariant is itself a
decomposition of that invariant.  Each step resolves the invariant into
finer admissible components, yet the components are constrained to
reassemble coherently.  The invariant appears as the stable residue of
its own refinement.

Repeated trial produced signal from accumulation.  Iterative
decomposition produces continuity from structure.  In both cases, the
invariant is not assumed at the outset.  It is certified by the
discipline of repetition.

The single invariant of the field therefore stands in the same logical
position as the mean of Chapter~2.  It is the value toward which lawful
refinement converges.  What repetition was to fact, iteration is to
representation: the engine that exhausts residue until only structure
remains.



\subsection{Elliptical Devices}

The preceding sections have isolated three components: the instrument,
which certifies admissible events; the inversion, which guarantees
recoverability; and the computational realization, which implements the
decoding rule.  What remains is to recognize the structure that binds
them.

\begin{definition}[Carrier]
A \emph{carrier} is the metaphysical coordination of an instrument, a 
computational model, and an inversion, consisting of
\begin{itemize}
\item an instrument certifying admissible events,
\item a computational realization implementing the encoding and
  decoding discipline,
\item an inversion guaranteeing recoverability of admissible symbols.
\end{itemize}
\end{definition}

In this regime, every admissible symbol corresponds to exactly one
ledger entry, and every ledger entry decodes to exactly one symbol.
There is no surplus of representation and no deficit of expression.
The phenomenal hypothesis is satisfied without approximation.

A carrier is not a laboratory artifact.  It is the implied
device required for the invariant to be representable at all.  If the
instrument certifies a single invariant, the carrier is the structure in
which that invariant can be exhaustively realized.  Nothing remains
outside the encoding.  Nothing is left to limit.

\paragraph{Computational closure.}
To realize a carrier one must supply the most expressive finite model
available.  The Turing device provides this closure: a finite alphabet,
a finite ledger, and a total transition map implementing the inversion.
Within finite bounds, no additional expressive power exists.  The model
exhausts the invariant rather than approximating it.

\paragraph{Regimes.}
The elliptical case is balanced.  Real devices are not balanced.  When
refinement introduces asymptotic tightening, when convergence replaces
finite exhaustion, the regime becomes parabolic.  When representation
outruns admissibility, producing symbols without recoverable invariant,
the regime becomes hyperbolic.

The carrier stands at the center.  It is the exact coincidence of
admissibility and representation.  Parabolic and hyperbolic devices are
deformations of this balance.  The theory therefore begins with the
elliptical case, where instrument, inversion, and computation close
without remainder.


A carrier is not a physical device.  It is the implied metaphysical
structure that satisfies the phenomenal hypothesis.  It is what must
exist for the invariant certified by the instrument to be representable
at all.

The elliptical case is the simplest realization of a carrier.  Here the
alphabet and ledger are finite and of equal cardinality.  The inversion
is bijective, and the computational model is total on its state space.
No admissible event lies outside the representable structure.

In this regime, the carrier is closed.  Refinement terminates because
every admissible distinction is already encoded.  Iteration stabilizes
after finitely many steps.  The invariant appears as a completed object
rather than as a limit.

\paragraph{The computational realization.}
To realize a carrier in the elliptical regime, one requires the most
expressive finite model available.  The Turing device supplies exactly
this: a finite alphabet, a finite ledger, and a total transition map
implementing the decoding rule.  Within finite bounds, it can simulate
any admissible computation.

Thus the computational model does not extend the invariant.  It exhausts
it.  The carrier, in the elliptical case, is the metaphysical closure of
instrument and inversion under maximal finite realization.

All subsequent regimes arise by relaxing this equality.  When the
alphabet exceeds the ledger, residue appears.  When the ledger exceeds
the alphabet, ambiguity emerges.  The elliptical carrier stands as the
balanced case in which admissibility and representation coincide.


\paragraph{5. Why a Carrier becomes necessary.}
Without further structure, distinct refinement branches produced by bisection
are incomparable. Each is locally consistent, yet no relation between them is
specified. The Carrier is introduced to coordinate these branches without
collapsing them into a single path. It functions as the shared support across
which all decompositions are defined. Importantly, the Carrier is not the limit
of refinement; it is the condition under which limits, if they exist, may be
compared. It holds the structural potential that refinement has not yet fixed.

\paragraph{6. The Carrier is not a continuum.}
It is tempting to identify the Carrier with a continuous substrate, but this
identification is neither necessary nor permitted. The Carrier carries no
metric, topology, or measure by default. What it provides is persistence under
partition: no matter how refinement proceeds, each admissible event refers to
some portion of the Carrier. The Carrier is therefore defined negatively, by
what refinement cannot eliminate. It is the non-vanishing residue of inversion
under repeated refinement.

\paragraph{7. Coordination, not completion.}
The essential role of the Carrier is coordination rather than completion. It
allows independently refined decompositions to be related without requiring
either to be exhaustive or final. This is why the Carrier appears precisely at
the junction of Cauchy and Cantor constructions. Cauchy iteration ensures local
coherence of symbols; Cantor decomposition ensures global coverage of
possibilities. The Carrier is the object with respect to which these two demands
are reconciled.

\paragraph{8. Recoverability and the Carrier.}
Recoverability imposes a strict constraint: no refinement may erase the
possibility of reconstructing earlier distinctions. In a Cantor decomposition,
this means that each refinement step must correspond to a partition of the same
underlying support. The Carrier is that support. It ensures that refinement
refines rather than replaces. For this reason, the Carrier is inseparable from
the instrument itself, being defined implicitly by its refinement rules and
admissibility constraints.

\paragraph{9. The Carrier as instrument-relative.}
Different instruments induce different Carriers. A Carrier is not a universal
substrate, but a consequence of a particular regime of coordination. Two
instruments addressing the same phenomenon may induce incompatible Carriers if
their admissibility criteria differ. This is not a contradiction, but a
reflection of the fact that Carriers are bookkeeping structures that stabilize
comparison. A single phenomenon may therefore admit multiple Carriers, none
privileged in isolation.

\paragraph{10. The Carrier as precondition for calibration.}
In summary, the Carrier arises when Cauchy iteration is required to coexist with
Cantor decomposition. It resolves the tension between local convergence and
global consistency without invoking completed infinities. Any subsequent act of
calibration presupposes such a structure, since comparison between refinement
histories is otherwise undefined. The Carrier is what remains fixed while
refinement proceeds, marking the point at which an instrument ceases to be
merely iterative and becomes eligible for calibration.


\section{Device}

A device is constructed from a phenomenal hypothesis. This hypothesis
is not a claim about ontology, but an operational assumption about stability:
that there exists a single phenomenon whose manifestations may be repeatedly
interrogated without contradiction. The device does not establish this
hypothesis; it presupposes it. Without such a presupposition, no measurement
protocol can even be formulated, as there would be no basis for expecting
coherence across trials.

The phenomenal hypothesis asserts that all admissible outcomes produced by
the device arise from a single underlying phenomenon. This phenomenon is not
itself recorded, nor is it directly observed. It functions instead as the
common explanatory source for the outcomes the device is designed to register.
In this sense, the phenomenon is prior to events, even though it can only be
justified retrospectively through them.

From the phenomenal hypothesis follows an enumeration of events. The
phenomenon itself is not enumerable, but each act of measurement produces a
discrete outcome. Repetition of the device's operation therefore induces a
countable sequence of events, indexed by the act of measurement rather than
by any intrinsic structure of the phenomenon. Enumeration here reflects the
finiteness of recording, not the finiteness of what is measured.

An event is the atomic outcome of a device measuring its hypothesized
phenomenon. Each event corresponds to a single admissible reception under the
device's constraints. Events are not selected freely by the device; they are
generated by the interaction between the phenomenon and the admissibility
structure imposed by calibration. In this way, phenomena generate events, but
only relative to a device capable of receiving them.

Each event carries a symbol drawn from a space $\sigma$, and the choice of
$\sigma$ is determined by the metaphysical content of the phenomenal
hypothesis. If the phenomenon is assumed to vary continuously, $\sigma$ may
be taken to be $\mathbb{R}$. If it is assumed discrete, $\sigma$ may be
finite or countable. The framework does not privilege any particular symbol
space. It requires only that the symbol space be consistent with the
hypothesis under which the device is constructed.

The symbol associated with an event is not required to admit ordering,
comparison, or metric structure. At the level of events, it is sufficient
that the symbol be receivable. Comparison, ordering, and quantitative
structure emerge only later, through repetition, correlance, and refinement
of the record. The event symbol therefore precedes mathematics in the
construction, even when it later admits mathematical representation.

Phenomena and events are purely metaphysical in the sense that they are not
written to the ledger. What is written is a record symbol, produced by the
device's representational apparatus. The event symbol belongs to the
metaphysical description of what is received; the record symbol belongs to
the physical and symbolic constraints of the device. Calibration exists to
manage the inevitable mismatch between these domains.

This separation between event and record is essential. If event symbols and
record symbols coincided, there would be no room for approximation, aliasing,
or noise. Measurement would collapse into trivial transcription. By allowing
the record to be less expressive than the event, the framework accommodates
finite resolution without sacrificing coherence.

Failure of reception does not produce contradiction. When a received symbol
cannot be reconciled with the device's admissibility structure, no event is
produced. The ledger does not advance. Such silence is not an error condition
but a boundary condition, marking the limits of the phenomenon the device was
constructed to measure. What cannot be stabilized simply does not enter the
record.

Although phenomena and events are not directly observable, they are not
arbitrary. Given a stable device and a sufficiently rich ledger, the
structure of the underlying phenomenon and the associated event space can be
reverse engineered, up to observational indistinguishability. This
reconstruction is constrained by calibration, admissibility, and
recoverability, not by metaphysical fiat.

In this way, the phenomenal hypothesis remains accountable. It is tested
indirectly by the coherence of the events generated under repetition and by
the stability of the records those events produce. A failed hypothesis
manifests not as a false statement, but as the inability to sustain
refinement without inconsistency or loss of admissibility.

The device therefore occupies a mediating role. It is constructed under a
metaphysical assumption, operates by producing events, and commits records
that are necessarily impoverished relative to the phenomenon itself. Yet from
this finite and partial output, the structure of the phenomenon can be
inferred, at least to the degree required for comparison and communication.

This section completes the transition from mechanism to meaning. A device
begins with a phenomenal hypothesis and ends with a ledger of records. Events
mediate between these domains, translating metaphysical structure into
admissible outcomes. What survives this translation under repetition is
elevated, in later chapters, to invariant structure and, ultimately, to law.

\section{Decomposition}

Decomposition is the procedure by which a device yields an invariant. In the
preceding section, a device was introduced as operating under a phenomenal
hypothesis: that its received symbols arise from a single coherent phenomenon.
Decomposition does not assert this hypothesis. It is the mechanism by which the
hypothesis is tested through operation.

To decompose a device is to exhibit an explicit coordination between the symbols
it receives and the states it records. This coordination is realized as a
pairing between an enumeration of event symbols and an enumeration of ledger
positions. The pairing is not an abstraction, but a concrete construction that
binds reception to record in a recoverable way.

The central object exposed by decomposition is the invariant. An invariant is
not a metaphysical constant, nor a hidden parameter of the phenomenon. It is the
residual structure that remains stable when the device is operated repeatedly
and its record refined. Decomposition isolates this residue by separating those
features of the device’s behavior that persist from those that vary.

Repetition is essential to this process. A single event carries no invariant
content. Only through an enumeration of events can one determine whether the
device’s coordination between symbol and record remains coherent. Decomposition
formalizes this test by asking whether new receptions can be reconciled with the
existing structure without loss of recoverability.

Admissibility plays a central role in this reconciliation. A received symbol is
admissible only if it can be interpreted through the reverse structure induced
by the decomposition. When a symbol cannot be so interpreted, no event is
produced and the ledger does not advance. Such silence marks the boundary of the
phenomenon the device was constructed to measure.

The admissibility relation is, in general, noncomputable. It makes a claim not
only about the symbol received, but about its coherence with the total structure
of the device and its accumulated record. Decomposition therefore marks the
boundary between the computable manipulation of lists and the noncomputable
judgment required by measurement.

The invariant produced by decomposition is retrospective and device-relative.
It is identified only after the device has been operated and its behavior
examined under refinement. What survives this refinement is retained as
invariant; what does not is discarded as irrelevant to the phenomenon.

Importantly, the invariant does not belong to individual events. Events may vary
widely within the admissible space of the phenomenon. The invariant belongs to
the device viewed across its operation. It summarizes what the device preserves,
not what it records in any single instance.

Decomposition also clarifies the role of noise. Noise is not defined by magnitude
or probability, but by irrelevance to the invariant. Variations in events that do
not disturb the invariant are noise relative to the device. Variations that do
disturb it signal a failure of the phenomenal hypothesis.

The reverse component of decomposition, often expressed as an inversion, plays a
validating role. While the device produces records, the inversion determines
which receptions may be coherently interpreted as events. Decomposition is
complete only when forward production and reverse interpretation are mutually
consistent.

At this stage, the invariant remains local. It is tied to a particular device
and a particular phenomenal hypothesis. Universality is not asserted. Different
devices may yield different invariants, even when their records are mutually
translatable. Decomposition establishes stability, not necessity.

Section 3.4 therefore completes the transition from measurement to structure. By
decomposing a device, we extract an invariant that is stable under refinement,
independent of individual events, and capable of surviving translation. This
invariant is the foundation upon which later notions of distance, motion, and
law are constructed.

\begin{coda}{Representational Noise}
\end{coda}
